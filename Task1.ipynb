{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kKzrUVrBpJn",
        "outputId": "e9168023-e022-42d5-9284-97af10c7c4de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_product_data(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
        "\n",
        "        for product in products:\n",
        "            product_url = \"https://www.amazon.in\" + product.find(\"a\", {\"class\": \"a-link-normal\"})[\"href\"]\n",
        "            product_name = product.find(\"span\", {\"class\": \"a-size-medium\"}).text.strip()\n",
        "            product_price = product.find(\"span\", {\"class\": \"a-price\"}).find(\"span\", {\"class\": \"a-offscreen\"}).text.strip()\n",
        "            product_rating = product.find(\"span\", {\"class\": \"a-icon-alt\"}).text.split()[0]\n",
        "            product_reviews = product.find(\"span\", {\"class\": \"a-size-base\"}).text.strip()\n",
        "\n",
        "            print(\"Product URL:\", product_url)\n",
        "            print(\"Product Name:\", product_name)\n",
        "            print(\"Product Price:\", product_price)\n",
        "            print(\"Rating:\", product_rating)\n",
        "            print(\"Number of Reviews:\", product_reviews)\n",
        "            print(\"--------------------------------------------------\")\n",
        "    else:\n",
        "        print(\"Error:\", response.status_code)\n",
        "\n",
        "def scrape_multiple_pages(base_url, num_pages):\n",
        "    for page in range(1, num_pages + 1):\n",
        "        url = base_url + f\"&page={page}\"\n",
        "        print(f\"Scraping Page {page}\")\n",
        "        get_product_data(url)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    base_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
        "    num_pages_to_scrape = 20\n",
        "    scrape_multiple_pages(base_url, num_pages_to_scrape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsnkYUHABsJL",
        "outputId": "d55a610f-0dc5-4efb-f821-2469fe8be4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Page 1\n",
            "Error: 503\n",
            "Scraping Page 2\n",
            "Error: 503\n",
            "Scraping Page 3\n",
            "Error: 503\n",
            "Scraping Page 4\n",
            "Error: 503\n",
            "Scraping Page 5\n",
            "Error: 503\n",
            "Scraping Page 6\n",
            "Error: 503\n",
            "Scraping Page 7\n",
            "Error: 503\n",
            "Scraping Page 8\n",
            "Error: 503\n",
            "Scraping Page 9\n",
            "Error: 503\n",
            "Scraping Page 10\n",
            "Error: 503\n",
            "Scraping Page 11\n",
            "Error: 503\n",
            "Scraping Page 12\n",
            "Error: 503\n",
            "Scraping Page 13\n",
            "Error: 503\n",
            "Scraping Page 14\n",
            "Error: 503\n",
            "Scraping Page 15\n",
            "Error: 503\n",
            "Scraping Page 16\n",
            "Error: 503\n",
            "Scraping Page 17\n",
            "Error: 503\n",
            "Scraping Page 18\n",
            "Error: 503\n",
            "Scraping Page 19\n",
            "Error: 503\n",
            "Scraping Page 20\n",
            "Error: 503\n"
          ]
        }
      ]
    }
  ]
}